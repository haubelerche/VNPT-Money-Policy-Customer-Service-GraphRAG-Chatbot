import logging
from typing import Optional, List

from schema import (
    Decision,
    DecisionType,
    RetrievedContext,
    FormattedResponse,
    ESCALATION_TEMPLATES,
    CLARIFICATION_QUESTIONS,
    FORBIDDEN_PHRASES,
    Config,
)

logger = logging.getLogger(__name__)


class ResponseGenerator:
    """T·∫°o c√¢u tr·∫£ l·ªùi d·ª±a tr√™n context v√† decision."""
    
    SYSTEM_PROMPT = """B·∫°n l√† Answer Formatter cho d·ªãch v·ª• h·ªó tr·ª£ VNPT Money.

QUY T·∫ÆC:
1. CH·ªà ƒê∆Ø·ª¢C s·ª≠ d·ª•ng n·ªôi dung trong <answer_content>
2. KH√îNG ƒê∆Ø·ª¢C th√™m th√¥ng tin m·ªõi
3. KH√îNG ƒê∆Ø·ª¢C suy ƒëo√°n tr·∫°ng th√°i giao d·ªãch c√° nh√¢n
4. Gi·ªØ nguy√™n √Ω nghƒ©a c·ªßa c√¢u tr·∫£ l·ªùi g·ªëc
5. Format t·ª± nhi√™n, th√¢n thi·ªán nh∆∞ng chuy√™n nghi·ªáp
6. N·∫øu c√≥ steps, format d·∫°ng numbered list
7. Kh√¥ng b·∫Øt ƒë·∫ßu b·∫±ng l·ªùi ch√†o
8. ƒêi th·∫≥ng v√†o n·ªôi dung tr·∫£ l·ªùi"""

    SYNTHESIS_PROMPT = """Tr·ª£ l√Ω VNPT Money. Tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin tham kh·∫£o.

C√ÇU H·ªéI: {user_question}

TH√îNG TIN THAM KH·∫¢O:
{contexts}

QUY T·∫ÆC:
- CH·ªà d√πng th√¥ng tin t·ª´ ngu·ªìn tham kh·∫£o, KH√îNG b·ªãa ƒë·∫∑t
- So s√°nh ng·ªØ nghƒ©a ƒë·ªÉ t√¨m ngu·ªìn ph√π h·ª£p (VD: "chuy·ªÉn t·ª´ ng√¢n h√†ng" = "n·∫°p ti·ªÅn v√†o v√≠")
- N·∫øu KH√îNG c√≥ th√¥ng tin ph√π h·ª£p ‚Üí tr·∫£ l·ªùi: "M√¨nh ch∆∞a c√≥ th√¥ng tin v·ªÅ v·∫•n ƒë·ªÅ n√†y. Vui l√≤ng li√™n h·ªá hotline 18001091 (nh√°nh 3)."
- Format: numbered list cho b∆∞·ªõc, bullet cho l∆∞u √Ω

Tr·∫£ l·ªùi:"""

    def __init__(self, llm_client):
        self.llm_client = llm_client
        self.model = Config.RESPONSE_GENERATOR_MODEL
        self.temperature = Config.RESPONSE_GENERATOR_TEMPERATURE
        self.max_tokens = Config.RESPONSE_GENERATOR_MAX_TOKENS
    
    def generate(
        self, 
        decision: Decision, 
        context: Optional[RetrievedContext], 
        user_question: str,
        all_contexts: Optional[List[RetrievedContext]] = None,
        need_account_lookup: bool = False
    ) -> FormattedResponse:
        # OPTIMIZATION: Skip LLM synthesis khi c√≥ context t·ªët ƒë·ªÉ gi·∫£m latency
        if decision.type in [DecisionType.DIRECT_ANSWER, DecisionType.ANSWER_WITH_CLARIFY]:
            # Ki·ªÉm tra n·∫øu top result c√≥ similarity cao (>= 0.85) ‚Üí d√πng direct answer (nhanh)
            use_direct = False
            similarity = 0.0
            
            if decision.top_result:
                similarity = decision.top_result.similarity_score
                if similarity >= 0.85:
                    use_direct = True
            
            if use_direct and context:
                # Fast path: Direct answer without LLM synthesis (~0.5s thay v√¨ 10-15s)
                logger.info(f"Fast path: Direct answer (similarity={similarity:.2f})")
                response = self._generate_direct_answer(decision, context, user_question)
                if need_account_lookup:
                    response = self._append_personal_escalation(response)
                return response
            elif all_contexts and len(all_contexts) > 0:
                # Slow path: LLM synthesis khi c·∫ßn t·ªïng h·ª£p nhi·ªÅu ngu·ªìn
                response = self._generate_synthesized_answer(decision, all_contexts, user_question)
                if need_account_lookup:
                    response = self._append_personal_escalation(response)
                return response
            elif context:
                response = self._generate_direct_answer(decision, context, user_question)
                if need_account_lookup:
                    response = self._append_personal_escalation(response)
                return response
            else:
                return self._generate_escalation_low_confidence()
        
        if decision.type == DecisionType.CLARIFY_REQUIRED:
            # Th·ª≠ d√πng LLM t·ªïng h·ª£p t·ª´ top contexts n·∫øu c√≥
            if all_contexts and len(all_contexts) >= 2:
                return self._generate_synthesized_answer(decision, all_contexts, user_question)
            return self._generate_clarification(decision, user_question)
        elif decision.type == DecisionType.ESCALATE_PERSONAL:
            return self._generate_escalation_personal(user_question)
        elif decision.type == DecisionType.ESCALATE_OUT_OF_SCOPE:
            return self._generate_escalation_out_of_scope()
        elif decision.type == DecisionType.ESCALATE_MAX_RETRY:
            return self._generate_escalation_max_retry()
        elif decision.type == DecisionType.ESCALATE_LOW_CONFIDENCE:
            return self._generate_escalation_low_confidence()
        else:
            logger.error(f"Unknown decision type: {decision.type}")
            return self._generate_escalation_low_confidence()
    
    def _generate_synthesized_answer(
        self, 
        decision: Decision, 
        contexts: List[RetrievedContext], 
        user_question: str
    ) -> FormattedResponse:
        """D√πng LLM t·ªïng h·ª£p c√¢u tr·∫£ l·ªùi t·ª´ nhi·ªÅu contexts."""
        try:
            # Build context string t·ª´ top 3 contexts (reduced for speed)
            context_parts = []
            for i, ctx in enumerate(contexts[:3]):
                part = f"--- Ngu·ªìn {i+1}: {ctx.problem_title or 'N/A'} ---\n"
                if ctx.answer_content:
                    part += ctx.answer_content
                if ctx.answer_steps:
                    steps = "\n".join(f"  {j+1}. {s}" for j, s in enumerate(ctx.answer_steps))
                    part += f"\nC√°c b∆∞·ªõc:\n{steps}"
                if ctx.answer_notes:
                    part += f"\nL∆∞u √Ω: {ctx.answer_notes}"
                context_parts.append(part)
            
            contexts_text = "\n\n".join(context_parts)
            
            prompt = self.SYNTHESIS_PROMPT.format(
                user_question=user_question,
                contexts=contexts_text
            )
            
            response_text = self._call_llm_synthesis(prompt)
            
            # Validate response
            if not response_text or len(response_text) < 20:
                logger.warning("LLM synthesis response too short, falling back")
                if contexts[0]:
                    return self._generate_direct_answer(decision, contexts[0], user_question)
                return self._generate_escalation_low_confidence()
            
            return FormattedResponse(
                message=response_text,
                source_citation="",
                decision_type=DecisionType.DIRECT_ANSWER
            )
            
        except Exception as e:
            logger.error(f"Synthesis failed: {e}")
            # Fallback to first context
            if contexts and contexts[0]:
                return self._generate_direct_answer(decision, contexts[0], user_question)
            return self._generate_escalation_low_confidence()
    
    def _call_llm_synthesis(self, prompt: str) -> str:
        """Call LLM for synthesis with specific settings."""
        try:
            response = self.llm_client.chat.completions.create(
                model=self.model,
                temperature=0.3,  # Lower temperature for more factual response
                max_tokens=self.max_tokens,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"LLM synthesis call failed: {e}")
            raise
    
    def _generate_direct_answer(self, decision: Decision, context: Optional[RetrievedContext], user_question: str) -> FormattedResponse:
        if not context:
            logger.warning("DIRECT_ANSWER kh√¥ng c√≥ context, fallback")
            return self._generate_escalation_low_confidence()
        response_text = self._format_answer_fast(context)
        return FormattedResponse(message=response_text, source_citation="", decision_type=DecisionType.DIRECT_ANSWER)
    
    def _format_answer_fast(self, context: RetrievedContext) -> str:
        parts = []
        if context.answer_content:
            parts.append(context.answer_content.strip())
        if context.answer_steps:
            steps_formatted = "\n".join(f"{i+1}. {step}" for i, step in enumerate(context.answer_steps))
            parts.append(f"\n**C√°c b∆∞·ªõc th·ª±c hi·ªán:**\n{steps_formatted}")
        if context.answer_notes:
            parts.append(f"\n**L∆∞u √Ω:** {context.answer_notes}")
        return "\n".join(parts)
    
    def _append_personal_escalation(self, response: FormattedResponse) -> FormattedResponse:
        """Th√™m th√¥ng tin escalation khi c·∫ßn tra so√°t giao d·ªãch c√° nh√¢n."""
        escalation_info = """

---
üìû **ƒê·ªÉ ki·ªÉm tra th√¥ng tin giao d·ªãch c·ª• th·ªÉ c·ªßa b·∫°n**, m√¨nh c·∫ßn chuy·ªÉn y√™u c·∫ßu ƒë·∫øn b·ªô ph·∫≠n h·ªó tr·ª£.

**üì± Hotline:** 18001091 (nh√°nh 3)
**üìç ƒêi·ªÉm giao d·ªãch:** C√°c c·ª≠a h√†ng VinaPhone tr√™n to√†n qu·ªëc

Khi li√™n h·ªá, vui l√≤ng cung c·∫•p:
‚Ä¢ S·ªë ƒëi·ªán tho·∫°i ƒëƒÉng k√Ω VNPT Money
‚Ä¢ Th·ªùi gian giao d·ªãch  
‚Ä¢ M√£ giao d·ªãch (n·∫øu c√≥)

T·ªïng ƒë√†i vi√™n s·∫Ω h·ªó tr·ª£ ki·ªÉm tra ngay cho b·∫°n."""
        
        new_message = response.message + escalation_info
        return FormattedResponse(
            message=new_message,
            source_citation=response.source_citation,
            decision_type=response.decision_type
        )
    
    def _generate_answer_with_clarify(self, decision: Decision, context: Optional[RetrievedContext], user_question: str) -> FormattedResponse:
        if not context:
            return self._generate_clarification(decision, user_question)
        answer_text = self._format_answer_fast(context)
        return FormattedResponse(message=answer_text, source_citation="", decision_type=DecisionType.ANSWER_WITH_CLARIFY)
    
    def _generate_clarification(self, decision: Decision, user_question: str) -> FormattedResponse:
        response_text = """M√¨nh ch∆∞a ch·∫Øc ch·∫Øn v·ªÅ v·∫•n ƒë·ªÅ b·∫°n ƒëang g·∫∑p ph·∫£i.

B·∫°n c√≥ th·ªÉ cho m√¨nh bi·∫øt th√™m:
- B·∫°n ƒëang th·ª±c hi·ªán giao d·ªãch g√¨? (n·∫°p ti·ªÅn, chuy·ªÉn ti·ªÅn, thanh to√°n...)
- C√≥ th√¥ng b√°o l·ªói c·ª• th·ªÉ n√†o kh√¥ng?
- Ho·∫∑c m√¥ t·∫£ chi ti·∫øt h∆°n t√¨nh hu·ªëng c·ªßa b·∫°n

M√¨nh s·∫Ω c·ªë g·∫Øng h·ªó tr·ª£ b·∫°n t·ªët nh·∫•t!"""
        return FormattedResponse(message=response_text, source_citation="", decision_type=DecisionType.CLARIFY_REQUIRED)
    
    def _generate_escalation_personal(self, user_question: str) -> FormattedResponse:
        message = ESCALATION_TEMPLATES["TEMPLATE_PERSONAL_DATA"]
        return FormattedResponse(message=message, source_citation="", decision_type=DecisionType.ESCALATE_PERSONAL)
    
    def _generate_escalation_out_of_scope(self) -> FormattedResponse:
        message = ESCALATION_TEMPLATES["TEMPLATE_OUT_OF_SCOPE"]
        return FormattedResponse(message=message, source_citation="", decision_type=DecisionType.ESCALATE_OUT_OF_SCOPE)
    
    def _generate_escalation_max_retry(self) -> FormattedResponse:
        message = ESCALATION_TEMPLATES["TEMPLATE_MAX_RETRY"]
        return FormattedResponse(message=message, source_citation="", decision_type=DecisionType.ESCALATE_MAX_RETRY)
    
    def _generate_escalation_low_confidence(self) -> FormattedResponse:
        message = ESCALATION_TEMPLATES["TEMPLATE_LOW_CONFIDENCE"]
        return FormattedResponse(message=message, source_citation="", decision_type=DecisionType.ESCALATE_LOW_CONFIDENCE)
    
    def _build_clarification_text(self, slots: List[str]) -> str:
        if not slots:
            return "B·∫°n c√≥ th·ªÉ m√¥ t·∫£ chi ti·∫øt h∆°n v·∫•n ƒë·ªÅ b·∫°n g·∫∑p ph·∫£i kh√¥ng?"
        questions = []
        for slot in slots[:2]:
            if slot in CLARIFICATION_QUESTIONS:
                questions.append(CLARIFICATION_QUESTIONS[slot])
        if not questions:
            return "B·∫°n c√≥ th·ªÉ m√¥ t·∫£ chi ti·∫øt h∆°n v·∫•n ƒë·ªÅ b·∫°n g·∫∑p ph·∫£i kh√¥ng?"
        return "\n".join(f"- {q}" for q in questions)
    
    def _call_llm(self, prompt: str) -> str:
        try:
            response = self.llm_client.chat.completions.create(
                model=self.model,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                messages=[
                    {"role": "system", "content": self.SYSTEM_PROMPT},
                    {"role": "user", "content": prompt}
                ]
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"LLM call th·∫•t b·∫°i: {e}")
            return "Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra. Vui l√≤ng th·ª≠ l·∫°i ho·∫∑c li√™n h·ªá hotline 18001091 (nh√°nh 3)"
    
    def _validate_response(self, response: str, source_content: str) -> str:
        response_lower = response.lower()
        for phrase in FORBIDDEN_PHRASES:
            if phrase in response_lower:
                logger.warning(f"Ph√°t hi·ªán c·ª•m t·ª´ b·ªã c·∫•m: {phrase}")
                return source_content
        return response


class ResponseGeneratorSimple:
    """Response generator ƒë∆°n gi·∫£n kh√¥ng d√πng LLM."""
    
    def generate(
        self, 
        decision: Decision, 
        context: Optional[RetrievedContext], 
        user_question: str,
        all_contexts: Optional[List[RetrievedContext]] = None
    ) -> FormattedResponse:
        if decision.type == DecisionType.DIRECT_ANSWER and context:
            message = context.answer_content
            if context.answer_steps:
                steps = "\n".join(f"{i+1}. {s}" for i, s in enumerate(context.answer_steps))
                message += f"\n\n**C√°c b∆∞·ªõc th·ª±c hi·ªán:**\n{steps}"
            citation = f"[Ref: {context.problem_id}/{context.answer_id}]"
        elif decision.type == DecisionType.ANSWER_WITH_CLARIFY and context:
            message = context.answer_content
            message += "\n\nB·∫°n c√≥ th·ªÉ cho m√¨nh bi·∫øt th√™m chi ti·∫øt v·ªÅ v·∫•n ƒë·ªÅ b·∫°n g·∫∑p ph·∫£i kh√¥ng?"
            citation = f"[Ref: {context.problem_id}/{context.answer_id}]"
        elif decision.type == DecisionType.CLARIFY_REQUIRED:
            clarify = self._build_clarification(decision.clarification_slots)
            message = f"ƒê·ªÉ h·ªó tr·ª£ b·∫°n t·ªët h∆°n, b·∫°n c√≥ th·ªÉ cho m√¨nh bi·∫øt:\n\n{clarify}"
            citation = ""
        elif decision.type == DecisionType.ESCALATE_PERSONAL:
            message = ESCALATION_TEMPLATES["TEMPLATE_PERSONAL_DATA"]
            citation = ""
        elif decision.type == DecisionType.ESCALATE_OUT_OF_SCOPE:
            message = ESCALATION_TEMPLATES["TEMPLATE_OUT_OF_SCOPE"]
            citation = ""
        elif decision.type == DecisionType.ESCALATE_MAX_RETRY:
            message = ESCALATION_TEMPLATES["TEMPLATE_MAX_RETRY"]
            citation = ""
        else:
            message = ESCALATION_TEMPLATES["TEMPLATE_LOW_CONFIDENCE"]
            citation = ""
        return FormattedResponse(message=message, source_citation=citation, decision_type=decision.type)
    
    def _build_clarification(self, slots: List[str]) -> str:
        if not slots:
            return "- B·∫°n ƒëang th·ª±c hi·ªán giao d·ªãch g√¨?\n- B·∫°n g·∫∑p v·∫•n ƒë·ªÅ c·ª• th·ªÉ g√¨?"
        questions = []
        for slot in slots[:2]:
            if slot in CLARIFICATION_QUESTIONS:
                questions.append(f"- {CLARIFICATION_QUESTIONS[slot]}")
        return "\n".join(questions) if questions else "- B·∫°n c√≥ th·ªÉ m√¥ t·∫£ chi ti·∫øt h∆°n kh√¥ng?"
